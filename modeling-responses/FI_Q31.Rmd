---
title: "Predictive modelling"
author: "George, John & William"
date: "11/17/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=F, message=FALSE, warning=FALSE}
source("Packages.R")
```

## Data Preparation and cleaning 
```{r, echo=F,message=FALSE, warning=FALSE, echo= F}
# PREPARATION AND CLEANING

load("FIHI_clean.RData")
data <- FIHI_sub2[,-1]
```


## Getting the missing values percentages

```{r,message=FALSE, warning=FALSE, echo= F}
# obtaining the missing percentage of each variable
missing_rate <- data.frame()
nr <- NROW(data)
nc <- NCOL(data)
Var_name <- variable.names(data)
for (i in 1:nc) {
  na <- sum(is.na(data[,i]))
  na_rate <- (na/nr)*100
  result <- list(Variable = Var_name[i],Number_Missing = na, Missing_Rate = na_rate 
                 )
  missing_rate <- rbind(missing_rate, result, stringsAsFactors = F)
}
kable(missing_rate, align = "lcc", caption = "Table : Missing values table displaying percentages") %>%
  kable_paper("hover", full_width = T)%>% 
       kable_styling(font_size = 12)
```

```{r}
missing_rate
```
The table above displays the number of missing values with missing percentages, from the output its clear that all variables have some amount of missing observation given they are all above 50%, hence a necessary missing data treatment is required.



```{r,message=FALSE, warning=FALSE, echo= F}
library(dplyr)
data_pa<- data %>%
  rename(college_school = `college/school`) %>%
  dplyr::select(-c(permanent_address,spent_night_elsewhere, FI_q26,FI_q28,FI_q27,FI_q30), -ends_with("_changed")) %>%
  filter(FI_q31 != "NA") 

dim(data_pa)
```

The response variables also contains missing values, hence we filter all missing observation with respect to each response variable out and used the remaining data set for further imputation and analysis analysis.


## missing value treatment
Method I
```{r,message=FALSE, warning=FALSE, echo= F}
#Missing  value imputation
set.seed(123)
suppressPackageStartupMessages(library(mice))
data_imputed <- mice(data_pa[,-18], printFlag = F)
data_1 <- complete(data_imputed, 1)
data1 <- as.data.frame(data_1)
data_pad<-cbind("FI_q31"=data_pa$FI_q31,data1)
rm(data_imputed, data_1, data1)
```

The mice package aided in imputation the missing values in the predictor variables, specifically by using the median, mice was chosen because it is robust to data and its imputation style.


# Treating imbalance classification
```{r}
library(ROSE)
 data_pad_balance<-ovun.sample(FI_q31 ~ ., data = data_pad, method = "both", p=0.5,                             N=NROW(data_pad), seed = 125)$data
 dim(data_pad_balance)
```

After an EDA on the selected responses variables, highly imbalanced classification was encountered. The imbalanced classification was treated with both sampling methods under the ROSE package.


## Converting predictors to category
```{r,message=FALSE, warning=FALSE, echo= F}
data_pad_balance <- data_pad_balance %>%
   mutate(across(everything(), as.factor))
```



## Partitioning data set
```{r,message=FALSE, warning=FALSE, echo= F}
# PARTITION DATA
set.seed(123)
intrain <- createDataPartition(y=1:NROW(data_pad_balance), p= 0.67, list = FALSE)
training <- data_pad_balance[intrain,];  testing <- data_pad_balance[-intrain,]

dim(training) # training data
dim(testing)
```

For the purpose of training and validation of each derived model ,and the estimating of the performance metrics, the entire data set was partitioned into training and testing in a ratio of 2/3 and 1/3 respectively.


## Model fitting 

Given our data set and its structure, the following supervised classification machine learning algorithm were employed to obtained a predictive model for each given response variable;

```{r}
#------ Model building -----------
# Create a wrapper function to abstract away the common aspects of model fitting
formula<- FI_q31~.
fit.model <- function(method, tunegrid="", data=NULL, formula=NULL) {
  
  data <- training
  if(is.null(formula)) formula<- FI_q31~.
  
  # Train the model
   train(
           formula,
           data = data,
           method = method,
           trControl = trainControl(method = "cv", 5),
           preProcess = c("center","scale"),
           tuneGrid = tunegrid)
          
}
```

```{r}
# Logistic Regression
log <-train(formula,
                 data=training,
                 method="glm",
                 family = binomial(link = "logit"),
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale")) 
```

```{r}
# LDA
lda <- train(formula,
                 data=training,
                 method="lda",
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"))

```


```{r}
#-------------- Elastic Net Models -------------------
# fit a LASSO model
lasso <- fit.model("glmnet", expand.grid(.alpha=1, .lambda=seq(0,0.1,0.01)))

# Fit a Ridge regression model
ridge <- fit.model("glmnet", expand.grid(.alpha=0, .lambda=seq(0,0.1,0.01)))

```

```{r}
# Bagging
# bag <- fit.model("rf", data.frame(mtry=11))
bag <- train(formula,
                 data=training,
                 method="rf",
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"),
                tuneGrid = data.frame(mtry=11),
                ntree = 1000) 

```

```{r}
# Random Forest
# rf <- fit.model("rf", data.frame(mtry=1:10))
# rf <- train(formula,
#                  data=training,
#                  method="rf",
#                 trControl = trainControl(method = "cv", 5),
#                 preProcess = c("center", "scale"),
#                 tuneGrid = data.frame(mtry=1:10),
#                 ntree = 1000) 

```

```{r}
#-------------- 

# Support Vector Machine with linear kernel
set.seed(125)
trctrl <- trainControl(method = "cv", number=5)
svc <- train(formula, data = training, method = "svmLinear",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)

```

```{r}
# Support Vector Machine with radial kernel
set.seed(125)
trctrl <- trainControl(method = "cv", number=5)
svmR <- train(formula , data = training, method = "svmRadial",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)
```


## Making predictions 

Metrics
```{r}
# Create a custom confusion matrix with performance metrics
metrics <- function(model_object, response="", test_data=NULL) {
  # response = "FI_q31"
  # model_object <- log
  # 
  if(is.null(test_data)) test_data <- testing
  
  # make predictions
  prediction <- predict(model_object, test_data) 
  
  target <- test_data[, response]
  
  cmat <- confusionMatrix(prediction, target, mode = "prec_recall")
  
  misscal<- round(mean(prediction != target),digits = 2)
  
 # Returned outputs
 return(list(
   accuracy = (1-misscal),
   mcr = misscal,
   sens = round(cmat$byClass[1],2),
   spec = round(cmat$byClass[2],2),
   fbeta = round(cmat$byClass[7],2)
 ))
  
}
```

```{r}
metric_log <- metrics(lasso, response = "FI_q31")
#------- Compute performance metrics for the full models ---------------
log.metric <- metrics(log, response = "FI_q31")
lda.metric <- metrics(lda, response = "FI_q31")
# knn.metric <- metrics(knn)
lasso.metric <- metrics(lasso, response = "FI_q31")
ridge.metric <- metrics(ridge, response = "FI_q31")
bag.metric <- metrics(bag, response = "FI_q31")
# rf.metric <- metrics(rf)
svc.metric <- metrics(svc, response = "FI_q31")
# svmP.metric <- metrics(svmP)
svmR.metric <- metrics(svmR, response = "FI_q31")

mod.sum <- data.frame(rbind(
                          c("Logistic", log.metric$mcr, log.metric$accuracy, log.metric$sens, log.metric$spec, log.metric$fbeta),
                          c("LDA",  lda.metric$mcr, lda.metric$accuracy, lda.metric$sens, lda.metric$spec, lda.metric$fbeta),
                          c("LASSO", lasso.metric$mcr, lasso.metric$accuracy, lasso.metric$sens, lasso.metric$spec, lasso.metric$fbeta),
                          c("Ridge", ridge.metric$mcr, ridge.metric$accuracy, ridge.metric$sens, ridge.metric$spec, ridge.metric$fbeta),
                          c("Bagging", bag.metric$mcr, bag.metric$accuracy, bag.metric$sens, bag.metric$spec, bag.metric$fbeta),
                          c("SVC",  svc.metric$mcr, svc.metric$accuracy, svc.metric$sens, svc.metric$spec, svc.metric$fbeta),
                          c("SVM (Radial Kernel)", svmR.metric$mcr, svmR.metric$accuracy, svmR.metric$sens, svmR.metric$spec, svmR.metric$fbeta)))

names(mod.sum) <- c("Model", "Misclassification Rate", "Accuracy", "Sensitivity", "Specificity", "fbeta")

kable(mod.sum, align = "lccccc", caption = "Table : Evaluation metrics for Housing Insecurity with Permanent Address as a response") %>%
  kable_paper("hover", full_width = F)%>% 
       kable_styling(font_size = 12)

```


Base on our table of results a best model is selected base on the performance metrics accuracy, misclassification and fbeta. That is a model with least misclassification , higher accuracy and an fbeta score approaching one. Hence SVM with radial basis function is the best model. 

```{r}

Var <- varImp(svmR, scale = FALSE)
plot(Var, main =
       "Variable of Important plot for hungry and didn't eat due to lack of money")

```
The plot above displays the variable of important base on our best model.






Base on our table of results the best model is selected based on the performance metrics accuracy, misclassification, and fbeta. That is a model with the least misclassification, higher accuracy, and an fbeta score approaching one. Considering the results from the housing insecurity models, the SVM with the radial basis kernel has the highest f\beta of 96% and the least misclassification rate of 0.4%, followed by the linear discriminate analysis with f\beta 72% and misclassification of 3%. Hence the support vector machine through the radial basis function is chosen as our best model. The variables are highly associated with predicting housing insecurity as displayed by the graph of the variable of important shows that total income, residence, and transportation reliability been the top variables with federal aid, gender, and weekly work hours being least of importance. However if a cutoff of say 50% is employed the best or associated variables will be the top three variables together with the head of household, living alone, and spending the night elsewhere. 

Furthermore, we observed that SVM performs better with a misclafficattion rate of 0.08, accuracy of 0.92, and f\beta of 0.92 respectively for the food insecurity. This is associated with the response Food insecurity 1. Using a cut of 50%, we can infer from the variable importance plot that, total income,  transport reliability and household are the top variables which is associated or predicts the response variable.
 
Also, we can observe that, support vector machine with misclaffication rate of 0.14, with accuracy of 0.86, and f\beta of 0.86 shows that, SVM performs better considering response variable: Food insecurity 3. Considering a cut of 50%, we can observe infer from the variable importance plot that total income, transport reliability and known homelessnes student predict or is associated with food insecurity 3. 

Finally, we can observe that, support vector machine with misclaffication rate of 0.07, with accuracy of 0.93, and f\beta of 0.93 shows that, SVM performs better considering response variable: Food insecurity 2 . Considering a cut of 50%, we can observe infer from the variable importance plot that total income, transport reliability and known_homelessnes_student predict or is associated with food insecurity. We can observe that, the total income, total reliability and known_homelessness_student are associated with both food insecurity 2  and 3.





 
















