---
title: "Predictive modelling"
author: "George, John & William"
date: "11/17/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=F, message=FALSE, warning=FALSE}
source("Packages.R")
```

## Data Preparation and cleaning 
```{r, echo=F,message=FALSE, warning=FALSE, echo= F}
# PREPARATION AND CLEANING

load("FIHI_clean.RData") 
data <- FIHI_sub2[,-1] # removing the id variable 
```


## Getting the missing values percentages

```{r,message=FALSE, warning=FALSE, echo= F}
# obtaining the missing percentage of each variable
missing_rate <- data.frame()
nr <- NROW(data)
nc <- NCOL(data)
Var_name <- variable.names(data)
for (i in 1:nc) {
  na <- sum(is.na(data[,i]))
  na_rate <- (na/nr)*100
  result <- list(Number_Missing = na, Missing_Rate = na_rate, 
                 Variable = Var_name[i])
  missing_rate <- rbind(missing_rate, result, stringsAsFactors = F)
}
head(missing_rate)
```

```{r,message=FALSE, warning=FALSE, echo= F}
library(dplyr)
data_pa<- data %>%
  dplyr::select(-starts_with("FI_"), -ends_with("_changed"), -`college/school`) %>%
  filter(permanent_address != "NA") 

dim(data_pa)
```


## missing value treatment

Method I
```{r,message=FALSE, warning=FALSE, echo= F}
#Missing  value imputation
set.seed(123)
suppressPackageStartupMessages(library(mice))
data_imputed <- mice(data_pa[,-15], printFlag = F)
data_1 <- complete(data_imputed, 1)
data1 <- as.data.frame(data_1)
data_pad<-cbind("permanent_address"=data_pa$permanent_address,data1)
rm(data_imputed, data_1, data1)
```

# Treating imbalance classification
```{r}
library(ROSE)
 data_pad_balance<-ovun.sample(permanent_address ~ ., data = data_pad, method = "both", p=0.5,N=NROW(data_pad), seed = 125)$data
 dim(data_pad_balance)
```

## Converting predictors to category
```{r,message=FALSE, warning=FALSE, echo= F}
data_pad_balance <- data_pad_balance %>%
   mutate(across(everything(), as.factor))
```

## Partitioning data set
```{r,message=FALSE, warning=FALSE, echo= F}
# PARTITION DATA
set.seed(123)
intrain <- createDataPartition(y=1:NROW(data_pad_balance), p= 0.67, list = FALSE)
training <- data_pad_balance[intrain,];  testing <- data_pad_balance[-intrain,]

dim(training) # training data
dim(testing)
```

The training data has 76 observations with 1887 now (old =1057 when compared) variables.
The testing data has 32 observation with 1887 now (old= 1057 when compared) variables.



## Model fitting 

```{r}
#------ Model building -----------
# Create a wrapper function to abstract away the common aspects of model fitting
formula<- permanent_address~.
fit.model <- function(method, tunegrid="", data=NULL, formula=NULL) {
  
  data <- training
  if(is.null(formula)) formula<- permanent_address~.
  
  # Train the model
   train(
           formula,
           data = data,
           method = method,
           trControl = trainControl(method = "cv", 5),
           preProcess = c("center","scale"),
           tuneGrid = tunegrid)
          
}
```

```{r}
# Logistic Regression
log <-train(formula,
                 data=training,
                 method="glm",
                 family = binomial(link = "logit"),
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale")) 
```

```{r}
# LDA
lda <- train(formula,
                 data=training,
                 method="lda",
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"))

```

```{r}
#-------------- Elastic Net Models -------------------
# fit a LASSO model
lasso <- fit.model("glmnet", expand.grid(.alpha=1, .lambda=seq(0,0.1,0.01)))

# Fit a Ridge regression model
ridge <- fit.model("glmnet", expand.grid(.alpha=0, .lambda=seq(0,0.1,0.01)))

```

```{r}
# Bagging
# bag <- fit.model("rf", data.frame(mtry=11))
bag <- train(formula,
                 data=training,
                 method="rf",
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"),
                tuneGrid = data.frame(mtry=11),
                ntree = 1000) 

```

```{r}
# Random Forest
# rf <- fit.model("rf", data.frame(mtry=1:10))
# rf <- train(formula,
#                  data=training,
#                  method="rf",
#                 trControl = trainControl(method = "cv", 5),
#                 preProcess = c("center", "scale"),
#                 tuneGrid = data.frame(mtry=1:10),
#                 ntree = 1000) 

```

```{r}
#-------------- 

# Support Vector Machine with linear kernel
set.seed(125)
trctrl <- trainControl(method = "cv", number=5)
svc <- train(formula, data = training, method = "svmLinear",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)

```

```{r}
# Support Vector Machine with radial kernel
set.seed(125)
trctrl <- trainControl(method = "cv", number=5)
svmR <- train(formula , data = training, method = "svmRadial",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)
```


## Making predictions 

Metrics
```{r}
library(mlr3measures)
# Create a custom confusion matrix with performance metrics
metrics <- function(model_object, response="", test_data=NULL) {
  # response = "permanent_address"
  # model_object <- log
  # 
  if(is.null(test_data)) test_data <- testing
  
  # make predictions
  prediction <- predict(model_object, test_data) 
  prediction_A <- predict(model_object, test_data, type="prob") 
  
  target <- test_data[, response]
  
  cmat <- confusionMatrix(prediction, target, mode = "prec_recall")
  ROC <- roc(target, predictor = prediction_A[,2])
  AUC_m<-round(ROC$auc, digits=4) 
  
  # Plotting the ROC_auc curves
  plot <- ggroc(ROC, colour = 'blue', size = 2) +
  ggtitle(paste0('(AUC = ', AUC_m, ')')) +
  theme(plot.title = element_text(hjust = 1))+ theme_minimal()
  
  misscal<- round(mean(prediction != target),digits = 2)
  
 # Returned outputs
 return(list(
   accuracy = (1-misscal),
   mcr = misscal,
   sens = round(cmat$byClass[1],2),
   spec = round(cmat$byClass[2],2),
   fbeta = round(cmat$byClass[7],2),
   auc = AUC_m,
   plot = plot
 ))
}
```


```{r}
metric_log <- metrics(lasso, response = "permanent_address")
#------- Compute performance metrics for the full models ---------------
log.metric <- metrics(log, response = "permanent_address")
lda.metric <- metrics(lda, response = "permanent_address")
# knn.metric <- metrics(knn)
lasso.metric <- metrics(lasso, response = "permanent_address")
ridge.metric <- metrics(ridge, response = "permanent_address")
bag.metric <- metrics(bag, response = "permanent_address")
# rf.metric <- metrics(rf)
svc.metric <- metrics(svc, response = "permanent_address")
# svmP.metric <- metrics(svmP)
svmR.metric <- metrics(svmR, response = "permanent_address")

mod.sum <- data.frame(rbind(
                          c("Logistic", log.metric$mcr, log.metric$accuracy, log.metric$sens, log.metric$spec, log.metric$fbeta, log.metric$auc),
                          c("LDA",  lda.metric$mcr, lda.metric$accuracy, lda.metric$sens, lda.metric$spec, lda.metric$fbeta, lda.metric$auc),
                          c("LASSO", lasso.metric$mcr, lasso.metric$accuracy, lasso.metric$sens, lasso.metric$spec, lasso.metric$fbeta, lasso.metric$auc),
                          c("Ridge", ridge.metric$mcr, ridge.metric$accuracy, ridge.metric$sens, ridge.metric$spec, ridge.metric$fbeta, ridge.metric$auc),
                          c("Bagging", bag.metric$mcr, bag.metric$accuracy, bag.metric$sens, bag.metric$spec, bag.metric$fbeta, bag.metric$auc),
                          c("SVC",  svc.metric$mcr, svc.metric$accuracy, svc.metric$sens, svc.metric$spec, svc.metric$fbeta, svc.metric$auc),
                          c("SVM (Radial Kernel)", svmR.metric$mcr, svmR.metric$accuracy, svmR.metric$sens, svmR.metric$spec, svmR.metric$fbeta, svmR.metric$auc)))

names(mod.sum) <- c("Model", "Misclassification Rate", "Accuracy", "Sensitivity", "Specificity", "fbeta", "AUC")

kable(mod.sum, align = "lcccccc", caption = "Table : Evaluation metrics for Housing Insecurity with Permanent Address as a response") %>%
  kable_paper("hover", full_width = F)%>% 
       kable_styling(font_size = 12)

```
Base on our table of results SVM with radial basis function is the best 

```{r}

Var <- varImp(svmR, scale = FALSE)
plot(Var)

```

## Graphing 

```{r}
library(ggpubr)
ggarrange(log.metric$plot, lda.metric$plot, lasso.metric$plot, ridge.metric$plot, svc.metric$plot, svmR.metric$plot + rremove("x.text"), 
          labels = c("log", "LDA", "Lasso", "Ridge","Linear","Radial"),
          ncol = 3, nrow = 2)

```




















