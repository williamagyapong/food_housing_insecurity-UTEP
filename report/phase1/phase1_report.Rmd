---
title: "Modeling Food and Housing Insecurity at UTEP - Phase 1"
subtitle: "Report"
author: 
 - "George E. Quaye"
 - "John Koomson"
 - "Willliam O. Agyapong"
  
affiliation: "Department of Mathematical Sciences, University of Texas at El Paso"
date: \center University of Texas, El Paso (UTEP)\center
       \center Department of Mathematical Sciences \center
output:
  bookdown::pdf_document2:
    fig_caption: yes
    keep_tex: no
    latex_engine: pdflatex
    number_sections: yes
    toc: yes
    toc_depth: 4
header-includes:
  - \usepackage{float}
  - \usepackage{setspace}
  - \doublespacing
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{amsfonts}
  - \usepackage{amsthm}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhf{}
  - \rhead{George Quaye, John Koomson, William Agyapong}
  - \lhead{Report - DS 6335}
  - \cfoot{\thepage}
  - \usepackage{algorithm}
  - \usepackage[noend]{algpseudocode}
geometry: margin = 0.8in
fontsize: 10pt
bibliography: ../../references.bib
link-citations: yes
linkcolor: blue
csl: ../../apa-6th-edition-no-ampersand.csl

---

```{r setup, warning=F, message=F, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=F, message=F, fig.pos = "!H", out.extra = "")

```

```{r, echo=F, message=FALSE, warning=FALSE}
 
# Load required packages and preprocessed data from the project root directory

source("../../packages.R")

load("../../FIHI_clean.RData")

```


\newpage


\section{Introduction}

In this section, we provide an overview of Food and Housing Insecurity in Colleges in the U.S. in addition to  the challenges faced by households in America. Also, we provide a description of the surveyed data obtained by The University of Texas at El Paso, processing of data cleaning and variable description.


This section  primarily provides a background to the study by means of an overview of food insecurity and housing insecurity in the US in general and at El Paso, the satellite region of our target population, in particular. 


## Overview of the study
This project is aimed at analyzing data based on a survey constructed in summer 2019 and 2020 by a team of researchers at UTEP (Moya, Crouse, Schober, Wagler) to assess the state of food and housing insecurity among UTEP students. A slightly modified version of the survey was administered summer 2020. In this work, we seek to assess whether there are any changes to food and housing insecurities since 2019. This is particularly important since the worldwide pandemic hit during this period. The primary and secondary research questions are listed below. 


1. **Primary Research Question**: What factors are associated with food insecurity (FI) and housing insecurity (HI) among UTEP students?

2. **Secondary Research Question**: Which subpopulations of students are most at risk for FI and HI at UTEP?

There are two phases to the project, phase I is concerned with the primary research question while phase II will be based on the secondary research question.


## Overview of Food Insecurity
Food insecurity is defined by the United States Department of Agriculture (USDA) as the lack of consistent access to enough food for active healthy life. It can also be defined as a physical discomfort, thus the definition can relate to an individual or a family's lack of resources to obtain enough food for a healthy living. With the increasing spread of COVID-19, and many Americans losing their jobs, one of America's challenge worsened: hunger.During this period, communities and colleges in America in particular faced the longest queue's for food compared to past years.

According to the USDA, 13.7 million or 10.5% of all households in the U.s. experienced food insecurity at some point during the COVID-19 pandemic. In addition, it was observed that, nearly 40% of college students were food insecure. That means,24 of every 60 students are food insecure. After several years of research on food insecurity, researchers discovered food insecurity as a likelihood to deter college students from completing their degree.

Further research from USDAS stated over the past 20 years and a study completed in 2017  showed that, racial minorities, first-generation students, low income students, student with children and LGBTQ+ students are highly affected by COVID-19. These variables were included in analyzing the factors affecting food insecurities at UTEP.

## Overview of Housing Insecurity ##

Housing insecurities includes  difficulty in paying for living expense, overcrowding, substandard living and frequently moving out. Though housing insecurities can affect anyone, lower income families are known to be highly impacted since they pay high proportion of their lower income on high cost rent.

According to a 2019 report by The Hope Center for college, Communities and Justice, nearly 3 out of 5 students experience having insecurities in 2018. Also, 14% of 4 year student reported experiencing homelessness. Despite the many challenges of housing insecurities, the impact of COVID-19 have increased the likelihood of homelessness among college students. For example, many students lost their jobs and financial due to COVID-19 pandemic.

\section{Data Preprocessing}

\subsection{ Data Description}

\subsection{Data cleaning and naming of variables}
In this section, we provide a brief description of the data for prediction and a method for renaming and collapsing the variables. The initial data from the survey comprises 12,536 observations with 104 columns. From the data, we observed enrollment and employment as the variables with the minimum number of missing values. In preparation of our data for modeling, we excluded the following questions from the our analysis: respondent ID, the age of the respondent, what pronouns do you use to describe yourself, how many persons depend on you, and where do you live. Table 1 shows the recoded names of the variables we considered in our analysis. 


```{r variables-description, echo=F}

var_names <- c("enrollment", "employment", "employment_type", "weekly_work_hrs", "ethnicity", "gender", "total_income", "academic_level", "college/school", "mode_transport", "transport_reliability", "living_alone", "dependents", "household_head", "residence", "permanent_address", "spent_night_elsewhere", "know_homelessness_studt", "federal_aid", "FI_q26", "FI_q27", "FI_q28", "FI_q30", "FI_q31", "expenditures_changed", "income_changed", "fed_aid_changed", "debt_changed")

# description = rep('brief description of the variable', length(var_names))

description = c(
  "Your enrollment at UTEP (Q1)",
  "Are you employ (Q2)",
  " You are consistenly working at which employment type (Q2)",
  "How many hours a week are you consistently working (Q3)",
  "What is your ethnicity (Q4)",
  "Which gender do you identify with",
  "The The estimated income for your household in 2019 (Q9)",
  "What is your academic level (Q10)",
  " Which College/School are you a student of (Q11)",
  "The common mode of commute (Q12)",
  "How reliable is it getting you to/from college (Q13)",
  "Do you live alone (Q14)",
  "Do you have dependents (Q15)",
  "Are you the head of the household (Q17)",
  "Where do you live (Q19)",
  " In the past 12 months, have you had a prmament adress (Q20)",
  " How frequently did you spent the night somewhere due to lack of housing (Q22)",
  "Do you know UTE students who have experienced homelessness (Q23)",
  "Have you received financial aid in the past 10 months (Q25)",
  " The food that I bought just didn't last. and I didn't have enough money to get more (Q26)",
  "I couldn't afford to eat balanced meal, how often (Q27)",
  " Did you ever cut the size of your meal (Q28)",
  " In the pst 12 months, have you eat less than you felt you should because there was not enough money (Q30)",
   " In the past 12 months, were you hungry but didn't eat because there wasn't enough money for food (Q31)",
"Since COVID-19, has there been any personal or household change in expenditures (Q32)",
"Since COVID-19, has there been any personal or household change in financial Income (Q33)",
"Since COVID-19, has there been any personal or household change in financial Aid (Q34)",
"Since COVID-19, has there been any personal or household change in debt (Q35)")

table1 <- data.frame(cbind(var_names, description))
names(table1) <- c("Variable Name", "Description")

kable(table1, caption = "Table 1: Table of recoded variable names from survey data")%>%
       kable_paper("hover", full_width = F)%>% 
       kable_styling(font_size = 12, latex_options = c("HOLD_position"))
```

\subsection{Collapsing the variables}

In other to obtain a feasible result, the following variables were collapsed due to insignificant proportion of respondent answering those questions. With regards to **Ethinicity**, we maintained Hispanic , Black/African American, White, Asian and collapsed American Indian, Native Hawaiian, mixed race and other into **other** . We decided not to keep a mixed race level because the total reduced to 88 after we  assigned (Hispanic, white) to Hispanic, and (Hispanic, Black) to Black. **Gender** was reduced to  3 levels: female, male, and non-binary/non-conforming. With respect to **Income** we kept first 5 levels and 100000 or more (11)->8, collapse (6,7)->6, (8,9,10)->7. For **Academic level**, we keep first 5, and collapse last 2 -> 6. For **College**, we maintain first 6 levels and colapse the remaining ones into other (level 7). For **Commute**, we keep car, Bus/public and other, and collapse car & carpool (2,3) -> 9, collapse bike, trolley, walk (5,6,7) -> 10. Keep Not applicable (0). For, **Federal Aid**, Emergency loan was reduced from 316 to 50, so we added those to loans(3).




\section{Analysis Plan}

This document serves to provide a road map to the approaches our group intend to adopt and execute in terms of our modeling approaches and general outline of our final report. We, however, remark that the various plans set forth are subject to changes according to recommendations and suggestions received from our domain experts (Dr. Amy Wagler and her research colleagues on this same project) upon consultations, and any other future modifications that we shall consider appropriate.

\subsection{Exploratory Data Analysis}

In this section, we will provide descriptive analyses of the data including numerical summaries and graphical displays as a first-hand insight into the data to help ourselves and our audiences to better understand the underlying dataset. We will primarily be looking at the distributions of our target variable (death event) and all the predictor variables.

In terms of the types of graphs or visualizations, I would like my group to consider creating comparative boxplots/histograms, segmented or comparative bar graphs  and, possibly, density plots.  We will critically assess these graphs and settle on a few which will help us to effectively tell a compelling story about the data. We will build animation effects into some of the graphs where possible or appropriate.

* **Cross Tabulations (Contingency Tables)**: Provide numerical summary of how responses differ by some categories. 

* **Comparative Boxplots/Histograms/Density plots**: These graphs will be used to describe the distribution of the continuous or numeric variables across the levels of the target variable.

* **Bar graphs (Segmented/Comparative bar graphs) and Mossaic Plots**: To explore how the available categorical predictors are distributed across the levels of the target or response variable.

* **Correlation plots** will be utilized to detect the presence of multicolinearity among the continuous predictors.

### Missing Data/Outliers and their treatments

The available data will be inspected for missing and unusual values. Any missing data and/or outlying data identified from our data preprocessing and exploratory analysis will be treated appropriately depending on the nature of the missingness and the information we are able to gather about the possible reasons for the missingness or unusual values. Possible methods to likely to be considered include imputation, listwise deletion, among others.


## Modeling Approaches

### Data Partitioning

For the purpose of predictions or testing on the machine learning algorithm models or estimation of the evaluation metrics for the best model selection, a division of the entire into say training data and testing data will be carried out. The training data would be used to fit the models and the testing data was used to derive the metrics for model selection or testing. This partitioning will be done by using the strategy of V -fold cross-validation on the misclassification errors, with V = n, where n would specified base on the sample size of the entire data set. We will randomly divide the data into V folds with stratification on the target variable category. This assures that similar proportions of 0s and 1s are preserved in each fold, and set.seed() will be used for easily reproducible results.


\subsubsection{Predictive Models}

In this part of the analysis, since the nature of the tasks present a classification problem, we will fit several classification models to the target variable as a function of all the possible predictors in the dataset that we will deem fit. All the models considered are *supervised* learning algorithms.

Candidate models include Logistic regression, Linear Discriminant Analysis (LDA), K-nearest neighbors (KNN), Support Vector Machines/Classifiers, Naive Bayes' algorithm, Multivariate Adaptive Regression Splines, Artificial Neural Network, and  Tree-based models such as Random Forest, Bagging, or Boosting. Due to the sensitivity of the KNN model to different scales, the continuous variables will be normalized and kept across the other models for consistency, that is, if we end up fiting a KNN model.


#### Logistic Regression

Logistic Regression is very simple and one of the mostly used traditional machine learning algorithms. It is a statistical model for predicting binary classes. However, as remarked by @james2013introduction, multiple-class extension of Logistic Regression exists but *Discriminant Analysis* models are commonly used in place of it. The dependent variable in here follows Bernoulli distribution. Logistic Regression model uses the logistic function or sigmoid function for predictive modeling of the given problem, which takes value between 0 and 1. 1 will be predicted if the curve goes to positive infinity and 0 if it goes to negative infinity. The logistic Regression model performs the predictive analysis based on the relationship between the binary dependent variable and the other one or more independent variables from the given dataset.

*Binary Logistic Regression, we will fit a regularized logistic regression model such as LASSO as one of the baseline classifiers for comparison. LASSO does not require the target variable to be normally distributed, no homogeneity of variance assumption is required, and with the aid of graph and output interpretation is effective under LASSO. LASSO, however, requires more data to achieve stability and is effective mostly on linearly separable.*


#### K-Nearest Neighbors Classifier

K-nearest neighbors regression (KNN regression) is one of the simplest and best-known
non-parametric methods. Unlike the Logistic Regression, no assumptions are made about the shape of the decision boundary. Therefore, we can expect this approach to dominate Logistic Regression when the decision boundary is highly non-linear. It uses a distance metric such as the Euclidean distance for separation between points in the feature space. In the KNN classification model, the prediction is purely based on neighbor data values without any assumption on the dataset. The $K$ in the name of the model represents the number of nearest neighbor data values. This parameter can be tuned to find an optimal value. Based on $K$, the decision is made by the KNN algorithm on classifying the given dataset. 


#### Classification Trees

According to [@james2013introduction], for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. In interpreting the results of a classification tree, we are often interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region. A recursive binary splitting is used to grow a classification tree. Classification trees are often preferred to other classifiers for the following reasons. One thing, they are non-linear classifiers which do not require the data to be linearly separable. Two, they are easy to read and very easy to explain to people. After a model is generated, it’s easy to report back to others regarding how the tree works. Moreover, trees can be displayed graphically, and are easily interpreted even bya non-expert (especially if they are small). Also, decision trees can easily handle qualitative predictors. However, trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree. For this reason, we will use tree models that help circumvent this challenge and greatly improves the predictive performance by aggregating many decision trees. Such tree models include Random Forest, Boosting, and Bagging. 

\paragraph{Random forest}

The random forest is a great predictive model for high dimensional data, and we will use RF as one of the baseline classifiers model. RF has high performance and accuracy with regards to modeling, provides feature importance of estimates, and can automatically handle missing values with no scaling required. However, given that RF has some advantages over other classifiers, its prediction time is high, can overfit the data, and also require more computational time and resources.

<!-- **Boosting** works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each
tree is fit on a modified version of the original data set. -->

#### Support Vector Machines (SVM)

Support Vector Machines; SVM is a great classification machine learning model for classifying data of clear classes. Also does not require a predetermined cutoff point for prediction. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximize the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall [@wikipedia].
We will consider using this classification model because it is versatile and effective in high dimensional spaces. They are versatile in the sense that different Kernel functions can be specified for the decision function. Common kernels include polynomial kernels and radial kernels. Support vector classifiers are also able to address the problem of possibly non-linear boundaries between classes. We will try at least three kernel SVM for this analysis noticeably, the Linear SVM and two Non-Linear kernels SVM such as the Gaussian radial basis function (RBF) and Polynomial kernel SVM through packages such as caret and kernlab.


#### Naive Bayes Classifier

It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. It works on Bayes theorem of probability to predict the class of unknown data sets. Naive Bayes algorithm is particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods. When the assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data. It should be however noted that this assumption of independent predictors may be unrealistic since, in real life, it is almost impossible that we get a set of predictors which are completely independent.  Another disadvantage of the Naive Bayes classifier which might discourage us from using it is that it works only with categorical variables, so one has to transform continuous features to discrete, which might lead to the loss of a lot of information.

#### Multivariate Adaptive Regression Splines

We will employ at least one MARS model with an appropriate set-up of parameters as another baseline classifier. The MARS algorithm is also suitable for a large number of predictor variables, robust to outliers, it automatically detects interactions between variables, and despite its complexity, it is an efficient fast algorithm. However, it is susceptible to overfitting, more difficult to understand and interpret as against others, and not good with missing data.


#### Artificial Neural Network

We will fit at least two different artificial neural networks (ANN) models, specifically with different numbers of layers and different numbers of units. ANN learning algorithms are quite robust o noise in the training data set, they are often used where the fast evaluation of the learned target function is required and its ability to learn and model non-linear and complex relationships. However, the ANN classifier has a disadvantage of unexplained functioning of the network and also no specific rule for determining the structure of artificial neural networks, the appropriate network structure is achieved through experience and trial and error.


## Model Validation

The k-fold cross-validation resampling technique will be used to validate our prediction models. 


## Performance/Evaluation Metrics

The following metrics will be employed to examine the performance of the models for best predictive power.

### F-beta Score ($F_\beta$)

The F-beta score or F-beta measure is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0. It turns the F-measure into a configurable single-score metric for evaluating a binary classification model based on the predictions made for the positive class. The beta parameter determines the weight of recall and precision in the combined score. $\beta < 1$ lends more weight to precision, while $\beta > 1$ favors recall. That is, a smaller beta value, such as 0.5, gives more weight to precision and less to recall, whereas a larger beta value, such as 2.0, gives less weight to precision and more weight to recall in the calculation of the score. It is a useful metric to use when both precision and recall are important but slightly more attention is needed on one or the other, such as when false negatives are more important than false positives, or the reverse (@MLM, @scikit_learn). 

We can compute the F-beta score by

$$
  F_\beta = \frac{(1 + \beta^2) * Precision * Recall}{\beta^2 * Precision + Recall},
$$
where **precision** and **recall** are defined below.

The choice of the beta parameter will be used in the name of the F-beta score. For example, a $\beta$ value of 1 is referred to as the $F_1$-measure or the $F_1$ score. A $\beta$ value of 2 is referred to as $F_2$-measure or $F_2$-score. 


**Precision**

Precision is a metric that quantifies the number of correct positive predictions made. It is computed as the ratio of correctly predicted positive classes divided by the total number of positive classes that were predicted as in the formula below.

$$
\text{Precision} = \frac{TP}{TP + FP} = \frac{\text{True Positives}}{\text{Total Predicted Positives}}
$$

**Recall**

Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made. The intuition for recall is that it is not concerned with false positives and it minimizes false negatives [@MLM]. It is calculated as

$$
\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{True Negatives}} = \frac{TP}{TP + NP}
$$



### Area Under the Receiver Operator Curve (AUROC)

The AUROC curve is a graphical illustration of the performance of the prediction model. The
ROC curve is the relationship between the recall and precision over varying threshold values. The threshold is the positive predictions of the model. The AUROC curve is plotted by keeping the x-axis a false positive rate and the y-axis as a true positive rate. Its value ranges from 0 to 1 [@theerthagiri2021prediction].


### Area Under the Precision Recall Curve (AUC-PR)

The area under the precision-recall curve (AUC-PR) is a model performance metric for binary responses that is appropriate for rare events and not dependent on model specificity. Precision-recall
curves have also been recognized as useful for classification performance assessment for unbalanced binary responses in bioinformatics. Like AUC-ROC, AUC-PR is a threshold-independent metric that calculates the area under a curve, where the curve is defined by a trade-off between different aspects of performance as the threshold applied to the model's predictions varies. Both ROC and PR curves are functions of the confusion matrix [@sofaer2019area].

### Model Selection
Base on the calculated metrics for each classifier algorithm, the best model will be selected, for instance, the model with the highest AUROC will be best among the others, however, AUROC alone is not the measure for selection, hence a contingent table will be obtained with classifier models on the row and metric on the column and base on all the metric estimates the one with the best performance across all metrics will be our final predictive model for analysis.

### Model Assessment
Our final obtained classifier model will be validated using the test data provide. That is we will use our model to predict on test data given, specifically looking at some important measures such as specificity and sensitivity of the predicted outcome by the model. Also, we seek to have a prediction accuracy of approximatley 98% and above for our model.


## Model Deployment

To aid easy deployment and implementation of our proposed model(s), we plan to develop a web application with the R Shiny App package.This dashboard will provide users, among other features, the flexibility to interact with the visualizations and models to assess performance based on changing parameters. We are aiming at an application similar to the online prediction tool developed by @dominguez2021bayesian. 



# Analysis and Results


## Exploratory Data Analysis

<!-- ## Data Preparation and cleaning  -->
```{r, echo=F,message=FALSE, warning=FALSE, echo= F}
# PREPARATION AND CLEANING

load("FIHI_clean.RData")
data <- FIHI_sub2[,-1]
```


### Inspecting missing values

```{r,message=FALSE, warning=FALSE, echo= F}
# obtaining the missing percentage of each variable
missing_rate <- data.frame()
nr <- NROW(data)
nc <- NCOL(data)
Var_name <- variable.names(data)
for (i in 1:nc) {
  na <- sum(is.na(data[,i]))
  na_rate <- (na/nr)*100
  result <- list(Variable = Var_name[i],Number_Missing = na, Missing_Rate = na_rate 
                 )
  missing_rate <- rbind(missing_rate, result, stringsAsFactors = F)
}
kable(missing_rate, align = "lcc", caption = "Table : Missing values table displaying percentages") %>%
  kable_paper("hover", full_width = T)%>% 
       kable_styling(font_size = 12, latex_options = c("HOLD_position"))
```


<!--## Summary Statistics -->
```{r}
# summary_stats <- as.data.frame(t(summary(FIHI_sub3[,-1])))
# kable(summary_stats)

# pa_tats <- FIHI_sub3 %>%
#   select(permanent_address) %>%
#   group_by(permanent_address) %>%
#   summarise(freq = n())
# names(pa_tats) <- c("Levels", "Obervations")
# 
# pa_tats2 <- FIHI_sub3 %>%
#   filter(permanent_address=="No") %>%
#   select(spent_night_elsewhere) %>%
#   group_by(spent_night_elsewhere) %>%
#   summarise(freq = n())
# names(pa_tats2) <- c("Levels", "Obervations")

# rbind(pa_tats, pa_tats2)
```

```{r plotting-functions}
# Make data for plotting

# For creating bar graphs of the response variables
mybarplot <- function(var, palette="Set2",
                      xlab="", ylab="",title="",dat='') 
{
  # palette=c("Set2","Set3","Accent","Set1","Paired","Dark2")
  ylab = ifelse(ylab=="", "Number of Respondents", ylab)
  
  if(!(class(dat)[1] %in% c('data.frame','tbl','tbl_df'))) dat <- FIHI_sub3

   if(ncol(dat) == 4)
   {
     # A plotting data was provided
     plotdf <- dat
   } else {
     # prepare data frame for plotting
      plotdf <- dat %>%
      group_by({{ var }}) %>%
      summarise(n=n()) %>%
      mutate(pct = n/sum(n),
             lbl = percent(pct)) 
   }
    
  # create the plot
  return(
    ggplot(plotdf, aes({{ var }},n, fill={{ var }})) +
    geom_bar(stat = "identity",position = "dodge")  +
    geom_text(aes(label = n), size=3, position = position_stack(vjust = 0.5)) +
    scale_fill_brewer(palette=palette) +
    labs(x=xlab, y=ylab, title=title) +
    theme_classic() +
    theme(legend.position = "none")
  )
}
# "Ever hungry but didn't eat?"
# mybarplot(FI_q31)
# mybarplot(spent_night_elsewhere, data = FIHI_sub3)

```


### Distribution of Housing Insecurity Responses


<!--**Figure 1** and **Figure2** present the distribution of the two housing insecurity variables available in the data set. From the two figures we observe -->

```{r, warning=F, message=F}

# Permanent Address
p1 <- mybarplot(permanent_address, 
                xlab = "Had permanent address in the past 12 months?"
                )

# Spending night elsewhere
plotdf <- FIHI_sub3 %>%
  filter(permanent_address=='No') %>%
  group_by(spent_night_elsewhere) %>%
  summarise(n=n()) %>% 
  mutate(pct = n/sum(n),
         lbl = percent(pct))
p2 <- mybarplot(spent_night_elsewhere, 
                xlab = "How fequently did you spend the night elsewhere?",
                dat = plotdf
                )
# Display plots side by side with the help of the patchwork package
p1 + p2

```

### Distribution of Food Insecurity Responses
```{r, warning=F, message=F}

# Q26: Food bought didn't last
p1 <- mybarplot(FI_q26, xlab = "Food bought did not last")

# Q27: Balanced diet
p2 <- mybarplot(FI_q27, xlab = "Couldn't afford balanced meals")

# Q28: 
p3 <- mybarplot(FI_q28, xlab = "Ever cut the size of meals?")

# Q3: 
p4 <- mybarplot(FI_q30, xlab = "Ever ate less than you should?")

# Q31
p5 <- mybarplot(FI_q31, xlab = "Ever got hungry but didn't eat?")
```

```{r}
p1 
```

```{r}

p2 
```

```{r}
(p3 + p4)
```

```{r}
p5

```


The table and graphs above display the number of missing values with missing percentages as well as the distribution of the potential response variables. From the results, the following observations were made:


* It is clear that all variables have some amount of missing observation given they are all above 50%, hence a necessary missing data treatment is required.

* The response variables also contains missing values, hence we filter all missing observation with respect to each response variable out and used the remaining data set for further imputation and analysis analysis.

* The mice package aided in imputation the missing values in the predictor variables, specifically by using the median, mice was chosen because it is robust to data and its imputation style.

* The selected responses variables exhibited highly imbalanced classification. The imbalanced classification was treated with both sampling methods under the ROSE package.


## Model Fitting  

Given our data set and its structure, the following supervised classification machine learning algorithm were employed to obtained a predictive model for each given response variable;

* Logistic Regression

* Linear Discriminant Analysis

* Least Absolute Shrinkage and Selection Operator

* Bagging

* Support Vector Machines with Linear Kernel

* Support Vector Machines with Radial Basis Kernel 



### Partitioning data set

For the purpose of training and validation of each derived model ,and the estimating of the performance metrics, the entire data set was partitioned into training and testing in a ratio of 2/3 and 1/3 respectively.

### Modeling Housing Insecurity

Here, we modeled housing insecurity as a response based whether or not the respondent had a permanent address.

```{r}

data_pa<- data %>%
  dplyr::select(-starts_with("FI_"), -ends_with("_changed"), -`college/school`) %>%
  filter(permanent_address != "NA") 

# dim(data_pa)



set.seed(123)
suppressPackageStartupMessages(library(mice))
data_imputed <- mice(data_pa[,-15], printFlag = F)
data_1 <- complete(data_imputed, 1)
data1 <- as.data.frame(data_1)
data_pad<-cbind("permanent_address"=data_pa$permanent_address,data1)
rm(data_imputed, data_1, data1)

# Treating imbalance classification


 data_pad_balance<-ovun.sample(permanent_address ~ ., data = data_pad, method = "both", p=0.5,                             N=NROW(data_pad), seed = 125)$data
 # dim(data_pad_balance)



data_pad_balance <- data_pad_balance %>%
   mutate(across(everything(), as.factor))


# PARTITION DATA
set.seed(123)
intrain <- createDataPartition(y=1:NROW(data_pad_balance), p= 0.67, list = FALSE)
training <- data_pad_balance[intrain,];  testing <- data_pad_balance[-intrain,]

# dim(training) # training data
# dim(testing)





#------ Model building -----------
# Create a wrapper function to abstract away the common aspects of model fitting
formula<- permanent_address~.
fit.model <- function(method, tunegrid="", data=NULL, formula=NULL) {
  
  data <- training
  if(is.null(formula)) formula<- permanent_address~.
  
  # Train the model
   train(
           formula,
           data = data,
           method = method,
           trControl = trainControl(method = "cv", 5),
           preProcess = c("center","scale"),
           tuneGrid = tunegrid)
          
}



# Logistic Regression
log <-train(formula,
                 data=training,
                 method="glm",
                 family = binomial(link = "logit"),
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale")) 



# LDA
lda <- train(formula,
                 data=training,
                 method="lda",
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"))




#-------------- Elastic Net Models -------------------
# fit a LASSO model
lasso <- fit.model("glmnet", expand.grid(.alpha=1, .lambda=seq(0,0.1,0.01)))

# Fit a Ridge regression model
ridge <- fit.model("glmnet", expand.grid(.alpha=0, .lambda=seq(0,0.1,0.01)))




# Bagging
# bag <- fit.model("rf", data.frame(mtry=11))
# bag <- train(formula,
#                  data=training,
#                  method="rf",
#                 trControl = trainControl(method = "cv", 5),
#                 preProcess = c("center", "scale"),
#                 tuneGrid = data.frame(mtry=11),
#                 ntree = 1000) 




#-------------- 

# Support Vector Machine with linear kernel
set.seed(125)
trctrl <- trainControl(method = "cv", number=5)
svc <- train(formula, data = training, method = "svmLinear",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)




# Support Vector Machine with radial kernel
set.seed(125)
trctrl <- trainControl(method = "cv", number=5)
svmR <- train(formula , data = training, method = "svmRadial",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)



## Making predictions 


# Create a custom confusion matrix with performance metrics
metrics <- function(model_object, response="", test_data=NULL) {
  # response = "permanent_address"
  # model_object <- log
  # 
  if(is.null(test_data)) test_data <- testing
  
  # make predictions
  prediction <- predict(model_object, test_data) 
  
  target <- test_data[, response]
  
  cmat <- confusionMatrix(prediction, target, mode = "prec_recall")
  
  misscal<- round(mean(prediction != target),digits = 2)
  
 # Returned outputs
 return(list(
   accuracy = (1-misscal),
   mcr = misscal,
   sens = round(cmat$byClass[1],2),
   spec = round(cmat$byClass[2],2),
   fbeta = round(cmat$byClass[7],2)
 ))
  
}

metric_log <- metrics(lasso, response = "permanent_address")
#------- Compute performance metrics for the full models ---------------
log.metric <- metrics(log, response = "permanent_address")
lda.metric <- metrics(lda, response = "permanent_address")
# knn.metric <- metrics(knn)
lasso.metric <- metrics(lasso, response = "permanent_address")
ridge.metric <- metrics(ridge, response = "permanent_address")
# bag.metric <- metrics(bag, response = "permanent_address")
# rf.metric <- metrics(rf)
svc.metric <- metrics(svc, response = "permanent_address")
# svmP.metric <- metrics(svmP)
svmR.metric <- metrics(svmR, response = "permanent_address")

mod.sum <- data.frame(rbind(
                          c("Logistic", log.metric$mcr, log.metric$accuracy, log.metric$sens, log.metric$spec, log.metric$fbeta),
                          c("LDA",  lda.metric$mcr, lda.metric$accuracy, lda.metric$sens, lda.metric$spec, lda.metric$fbeta),
                          c("LASSO", lasso.metric$mcr, lasso.metric$accuracy, lasso.metric$sens, lasso.metric$spec, lasso.metric$fbeta),
                          c("Ridge", ridge.metric$mcr, ridge.metric$accuracy, ridge.metric$sens, ridge.metric$spec, ridge.metric$fbeta),
                          # c("Bagging", bag.metric$mcr, bag.metric$accuracy, bag.metric$sens, bag.metric$spec, bag.metric$fbeta),
                          c("SVC",  svc.metric$mcr, svc.metric$accuracy, svc.metric$sens, svc.metric$spec, svc.metric$fbeta),
                          c("SVM (Radial Kernel)", svmR.metric$mcr, svmR.metric$accuracy, svmR.metric$sens, svmR.metric$spec, svmR.metric$fbeta)))

names(mod.sum) <- c("Model", "Misclassification Rate", "Accuracy", "Sensitivity", "Specificity", "fbeta")

kable(mod.sum, align = "lccccc", caption = "Table : Evaluation metrics for Housing Insecurity with Permanent Address as a response") %>%
  kable_paper("hover", full_width = F)%>% 
       kable_styling(font_size = 12, latex_options = c("HOLD_position"))




Var <- varImp(svmR, scale = FALSE)
plot(Var, main ="Figure: Variable of Importance")

######

```


### Modeling Food Insecurity

In this section, we report results for modeling food insecurity in terms response variables based on survey questions as described below:

* Food Insecurity 1:  In the last 12 months, since (today's date), did you ever cut the size of your meals or skip meals because there was not enough money for food?

* Food Insecurity 2:  In the last 12 months, did you ever eat less than you felt you should because there was not enough money for food?

* Food Insecurity 3: In the past 12 months, were you ever hungry but didn't eat because there wasn't enough money for food?

#### Results for Food Insecurity Response 1

```{r, warning=F, message=F}
data_pa<- data %>%
  rename(college_school = `college/school`) %>%
  dplyr::select(-c(permanent_address,spent_night_elsewhere, FI_q26, FI_q27,FI_q30, FI_q31), -ends_with("_changed")) %>%
  filter(data$FI_q28 != "NA") 

#Missing  value imputation
set.seed(123)
suppressPackageStartupMessages(library(mice))
data_imputed <- mice(data_pa[,-which(names(data_pa)=='FI_q28')], printFlag = F)
data_1 <- complete(data_imputed, 1)
data1 <- as.data.frame(data_1)
data_pad<-cbind("FI_q28"=data_pa$FI_q28,data1)
rm(data_imputed, data_1, data1)

# Treating imbalance classification
data_pad_balance<-ovun.sample(FI_q28 ~ ., data = data_pad, method = "both", p=0.5,                             N=NROW(data_pad), seed = 125)$data

## Converting predictors to category
data_pad_balance <- data_pad_balance %>%
   mutate(across(everything(), as.factor))


## Partitioning data set

# PARTITION DATA

intrain <- createDataPartition(y=1:NROW(data_pad_balance), p= 0.67, list = FALSE)
training <- data_pad_balance[intrain,];  testing <- data_pad_balance[-intrain,]

# dim(training) # training data
# dim(testing)

#------ Model building -----------
# Create a wrapper function to abstract away the common aspects of model fitting
formula<- FI_q28~.
fit.model <- function(method, tunegrid="", data=NULL, formula=NULL) {
  
  data <- training
  if(is.null(formula)) formula<- FI_q28~.
  
  # Train the model
   train(
           formula,
           data = data,
           method = method,
           trControl = trainControl(method = "cv", 5),
           preProcess = c("center","scale"),
           tuneGrid = tunegrid)
          
}


# Logistic Regression
log <-train(formula,
                 data=training,
                 method="glm",
                 family = binomial(link = "logit"),
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale")) 



# LDA
lda <- train(formula,
                 data=training,
                 method="lda",
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"))



#-------------- Elastic Net Models -------------------
# fit a LASSO model
lasso <- fit.model("glmnet", expand.grid(.alpha=1, .lambda=seq(0,0.1,0.01)))

# Fit a Ridge regression model
ridge <- fit.model("glmnet", expand.grid(.alpha=0, .lambda=seq(0,0.1,0.01)))


# Bagging
# bag <- fit.model("rf", data.frame(mtry=11))
# bag <- train(formula,
#                  data=training,
#                  method="rf",
#                 trControl = trainControl(method = "cv", 5),
#                 preProcess = c("center", "scale"),
#                 tuneGrid = data.frame(mtry=11),
#                 ntree = 1000) 




# Random Forest
# rf <- fit.model("rf", data.frame(mtry=1:10))
# rf <- train(formula,
#                  data=training,
#                  method="rf",
#                 trControl = trainControl(method = "cv", 5),
#                 preProcess = c("center", "scale"),
#                 tuneGrid = data.frame(mtry=1:10),
#                 ntree = 1000) 




#-------------- 

# Support Vector Machine with linear kernel

trctrl <- trainControl(method = "cv", number=5)
svc <- train(formula, data = training, method = "svmLinear",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)



# Support Vector Machine with radial kernel
trctrl <- trainControl(method = "cv", number=5)
svmR <- train(formula , data = training, method = "svmRadial",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)




# Create a custom confusion matrix with performance metrics
metrics <- function(model_object, response="", test_data=NULL) {
  # response = "FI_q28"
  # model_object <- log
  # 
  if(is.null(test_data)) test_data <- testing
  
  # make predictions
  prediction <- predict(model_object, test_data) 
  
  target <- test_data[, response]
  
  cmat <- confusionMatrix(prediction, target, mode = "prec_recall")
  
  misscal<- round(mean(prediction != target),digits = 2)
  
 # Returned outputs
 return(list(
   accuracy = (1-misscal),
   mcr = misscal,
   sens = round(cmat$byClass[1],2),
   spec = round(cmat$byClass[2],2),
   fbeta = round(cmat$byClass[7],2)
 ))
  
}



metric_log <- metrics(lasso, response = "FI_q28")
#------- Compute performance metrics for the full models ---------------
log.metric <- metrics(log, response = "FI_q28")
lda.metric <- metrics(lda, response = "FI_q28")
# knn.metric <- metrics(knn)
lasso.metric <- metrics(lasso, response = "FI_q28")
ridge.metric <- metrics(ridge, response = "FI_q28")
# bag.metric <- metrics(bag, response = "FI_q28")
# rf.metric <- metrics(rf)
svc.metric <- metrics(svc, response = "FI_q28")
# svmP.metric <- metrics(svmP)
svmR.metric <- metrics(svmR, response = "FI_q28")

mod.sum <- data.frame(rbind(
                          c("Logistic", log.metric$mcr, log.metric$accuracy, log.metric$sens, log.metric$spec, log.metric$fbeta),
                          c("LDA",  lda.metric$mcr, lda.metric$accuracy, lda.metric$sens, lda.metric$spec, lda.metric$fbeta),
                          c("LASSO", lasso.metric$mcr, lasso.metric$accuracy, lasso.metric$sens, lasso.metric$spec, lasso.metric$fbeta),
                          c("Ridge", ridge.metric$mcr, ridge.metric$accuracy, ridge.metric$sens, ridge.metric$spec, ridge.metric$fbeta),
                          # c("Bagging", bag.metric$mcr, bag.metric$accuracy, bag.metric$sens, bag.metric$spec, bag.metric$fbeta),
                          c("SVC",  svc.metric$mcr, svc.metric$accuracy, svc.metric$sens, svc.metric$spec, svc.metric$fbeta),
                          c("SVM (Radial Kernel)", svmR.metric$mcr, svmR.metric$accuracy, svmR.metric$sens, svmR.metric$spec, svmR.metric$fbeta)))

names(mod.sum) <- c("Model", "Misclassification Rate", "Accuracy", "Sensitivity", "Specificity", "fbeta")

kable(mod.sum, align = "lccccc", caption = "Table : Evaluation metrics for  food insecurity 1 as a response") %>%
  kable_paper("hover", full_width = F)%>% 
       kable_styling(font_size = 12, latex_options = c("HOLD_position"))


Var <- varImp(svmR, scale = FALSE)
plot(Var, main = "Figure: Variable of Importance")



```


#### Results for Food Insecurity Response 2

```{r, warning=F, message=F}

data_pa<- data %>%
  rename(college_school = `college/school`) %>%
  dplyr::select(-c(permanent_address,spent_night_elsewhere, FI_q26, FI_q27, FI_q28,FI_q31), -ends_with("_changed")) %>%
  filter(FI_q30 != "NA") 

# dim(data_pa)



## missing value treatment

#Missing  value imputation
set.seed(123)
suppressPackageStartupMessages(library(mice))
data_imputed <- mice(data_pa[,-which(names(data_pa)=='FI_q30')], printFlag = F)
data_1 <- complete(data_imputed, 1)
data1 <- as.data.frame(data_1)
data_pad<-cbind("FI_q30"=data_pa$FI_q30,data1)
rm(data_imputed, data_1, data1)


# Treating imbalance classification

# library(ROSE)
 data_pad_balance<-ovun.sample(FI_q30 ~ ., data = data_pad, method = "both", p=0.5,                             N=NROW(data_pad), seed = 125)$data
 # dim(data_pad_balance)


## Converting predictors to category

data_pad_balance <- data_pad_balance %>%
   mutate(across(everything(), as.factor))


## Partitioning data set

# PARTITION DATA
set.seed(123)
intrain <- createDataPartition(y=1:NROW(data_pad_balance), p= 0.67, list = FALSE)
training <- data_pad_balance[intrain,];  testing <- data_pad_balance[-intrain,]

# dim(training) # training data
# dim(testing)


#------ Model building -----------
# Create a wrapper function to abstract away the common aspects of model fitting
formula<- FI_q30~.
fit.model <- function(method, tunegrid="", data=NULL, formula=NULL) {
  
  data <- training
  if(is.null(formula)) formula<- FI_q30~.
  
  # Train the model
   train(
           formula,
           data = data,
           method = method,
           trControl = trainControl(method = "cv", 5),
           preProcess = c("center","scale"),
           tuneGrid = tunegrid)
          
}



# Logistic Regression
log <-train(formula,
                 data=training,
                 method="glm",
                 family = binomial(link = "logit"),
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale")) 

# LDA
lda <- train(formula,
                 data=training,
                 method="lda",
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"))





#-------------- Elastic Net Models -------------------
# fit a LASSO model
lasso <- fit.model("glmnet", expand.grid(.alpha=1, .lambda=seq(0,0.1,0.01)))

# Fit a Ridge regression model
ridge <- fit.model("glmnet", expand.grid(.alpha=0, .lambda=seq(0,0.1,0.01)))


# Bagging
# bag <- fit.model("rf", data.frame(mtry=11))
# bag <- train(formula,
#                  data=training,
#                  method="rf",
#                 trControl = trainControl(method = "cv", 5),
#                 preProcess = c("center", "scale"),
#                 tuneGrid = data.frame(mtry=11),
#                 ntree = 1000) 



#-------------- 

# Support Vector Machine with linear kernel
set.seed(125)
trctrl <- trainControl(method = "cv", number=5)
svc <- train(formula, data = training, method = "svmLinear",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)




# Support Vector Machine with radial kernel
set.seed(125)
trctrl <- trainControl(method = "cv", number=5)
svmR <- train(formula , data = training, method = "svmRadial",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)



## Making predictions 

# Create a custom confusion matrix with performance metrics
metrics <- function(model_object, response="", test_data=NULL) {
  # response = "FI_q30"
  # model_object <- log
  # 
  if(is.null(test_data)) test_data <- testing
  
  # make predictions
  prediction <- predict(model_object, test_data) 
  
  target <- test_data[, response]
  
  cmat <- confusionMatrix(prediction, target, mode = "prec_recall")
  
  misscal<- round(mean(prediction != target),digits = 2)
  
 # Returned outputs
 return(list(
   accuracy = (1-misscal),
   mcr = misscal,
   sens = round(cmat$byClass[1],2),
   spec = round(cmat$byClass[2],2),
   fbeta = round(cmat$byClass[7],2)
 ))
  
}


metric_log <- metrics(lasso, response = "FI_q30")
#------- Compute performance metrics for the full models ---------------
log.metric <- metrics(log, response = "FI_q30")
lda.metric <- metrics(lda, response = "FI_q30")
# knn.metric <- metrics(knn)
lasso.metric <- metrics(lasso, response = "FI_q30")
ridge.metric <- metrics(ridge, response = "FI_q30")
# bag.metric <- metrics(bag, response = "FI_q30")
# rf.metric <- metrics(rf)
svc.metric <- metrics(svc, response = "FI_q30")
# svmP.metric <- metrics(svmP)
svmR.metric <- metrics(svmR, response = "FI_q30")

mod.sum <- data.frame(rbind(
                          c("Logistic", log.metric$mcr, log.metric$accuracy, log.metric$sens, log.metric$spec, log.metric$fbeta),
                          c("LDA",  lda.metric$mcr, lda.metric$accuracy, lda.metric$sens, lda.metric$spec, lda.metric$fbeta),
                          c("LASSO", lasso.metric$mcr, lasso.metric$accuracy, lasso.metric$sens, lasso.metric$spec, lasso.metric$fbeta),
                          c("Ridge", ridge.metric$mcr, ridge.metric$accuracy, ridge.metric$sens, ridge.metric$spec, ridge.metric$fbeta),
                          # c("Bagging", bag.metric$mcr, bag.metric$accuracy, bag.metric$sens, bag.metric$spec, bag.metric$fbeta),
                          c("SVC",  svc.metric$mcr, svc.metric$accuracy, svc.metric$sens, svc.metric$spec, svc.metric$fbeta),
                          c("SVM (Radial Kernel)", svmR.metric$mcr, svmR.metric$accuracy, svmR.metric$sens, svmR.metric$spec, svmR.metric$fbeta)))

names(mod.sum) <- c("Model", "Misclassification Rate", "Accuracy", "Sensitivity", "Specificity", "fbeta")

kable(mod.sum, align = "lccccc", caption = "Table : Evaluation metrics for  food insecurity 2 as a response") %>%
  kable_paper("hover", full_width = F)%>% 
       kable_styling(font_size = 12, latex_options = c("HOLD_position"))





Var <- varImp(svmR, scale = FALSE)
plot(Var, main = "Figure: Variable of Importance")

```


#### Results for Food Insecurity Response 3

```{r, warning=F, message=F}

data_pa<- data %>%
  rename(college_school = `college/school`) %>%
  dplyr::select(-c(permanent_address,spent_night_elsewhere, FI_q26,FI_q28,FI_q27,FI_q30), -ends_with("_changed")) %>%
  filter(FI_q31 != "NA") 

# dim(data_pa)


## missing value treatment

#Missing  value imputation
set.seed(123)
suppressPackageStartupMessages(library(mice))
data_imputed <- mice(data_pa[,-18], printFlag = F)
data_1 <- complete(data_imputed, 1)
data1 <- as.data.frame(data_1)
data_pad<-cbind("FI_q31"=data_pa$FI_q31,data1)
rm(data_imputed, data_1, data1)


# Treating imbalance classification

#library(ROSE)
 data_pad_balance<-ovun.sample(FI_q31 ~ ., data = data_pad, method = "both", p=0.5,                             N=NROW(data_pad), seed = 125)$data
 dim(data_pad_balance)


## Converting predictors to category

data_pad_balance <- data_pad_balance %>%
   mutate(across(everything(), as.factor))


## Partitioning data set

# PARTITION DATA

intrain <- createDataPartition(y=1:NROW(data_pad_balance), p= 0.67, list = FALSE)
training <- data_pad_balance[intrain,];  testing <- data_pad_balance[-intrain,]

# dim(training) # training data
# dim(testing)



## Model fitting 


#------ Model building -----------
# Create a wrapper function to abstract away the common aspects of model fitting
formula<- FI_q31~.
fit.model <- function(method, tunegrid="", data=NULL, formula=NULL) {
  
  data <- training
  if(is.null(formula)) formula<- FI_q31~.
  
  # Train the model
   train(
           formula,
           data = data,
           method = method,
           trControl = trainControl(method = "cv", 5),
           preProcess = c("center","scale"),
           tuneGrid = tunegrid)
          
}

# Logistic Regression
log <-train(formula,
                 data=training,
                 method="glm",
                 family = binomial(link = "logit"),
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale")) 

# LDA
lda <- train(formula,
                 data=training,
                 method="lda",
                trControl = trainControl(method = "cv", 5),
                preProcess = c("center", "scale"))


#-------------- Elastic Net Models -------------------
# fit a LASSO model
lasso <- fit.model("glmnet", expand.grid(.alpha=1, .lambda=seq(0,0.1,0.01)))

# Fit a Ridge regression model
ridge <- fit.model("glmnet", expand.grid(.alpha=0, .lambda=seq(0,0.1,0.01)))


# Bagging
# bag <- fit.model("rf", data.frame(mtry=11))
# bag <- train(formula,
#                  data=training,
#                  method="rf",
#                 trControl = trainControl(method = "cv", 5),
#                 preProcess = c("center", "scale"),
#                 tuneGrid = data.frame(mtry=11),
#                 ntree = 1000) 

#-------------- 

# Support Vector Machine with linear kernel

trctrl <- trainControl(method = "cv", number=5)
svc <- train(formula, data = training, method = "svmLinear",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)


# Support Vector Machine with radial kernel
trctrl <- trainControl(method = "cv", number=5)
svmR <- train(formula , data = training, method = "svmRadial",
                    trControl=trctrl, prob.model=T,
                    tuneLength = 10)



## Making predictions 

# Create a custom confusion matrix with performance metrics
metrics <- function(model_object, response="", test_data=NULL) {
  # response = "FI_q31"
  # model_object <- log
  # 
  if(is.null(test_data)) test_data <- testing
  
  # make predictions
  prediction <- predict(model_object, test_data) 
  
  target <- test_data[, response]
  
  cmat <- confusionMatrix(prediction, target, mode = "prec_recall")
  
  misscal<- round(mean(prediction != target),digits = 2)
  
 # Returned outputs
 return(list(
   accuracy = (1-misscal),
   mcr = misscal,
   sens = round(cmat$byClass[1],2),
   spec = round(cmat$byClass[2],2),
   fbeta = round(cmat$byClass[7],2)
 ))
  
}


metric_log <- metrics(lasso, response = "FI_q31")
#------- Compute performance metrics for the full models ---------------
log.metric <- metrics(log, response = "FI_q31")
lda.metric <- metrics(lda, response = "FI_q31")
# knn.metric <- metrics(knn)
lasso.metric <- metrics(lasso, response = "FI_q31")
ridge.metric <- metrics(ridge, response = "FI_q31")
# bag.metric <- metrics(bag, response = "FI_q31")
# rf.metric <- metrics(rf)
svc.metric <- metrics(svc, response = "FI_q31")
# svmP.metric <- metrics(svmP)
svmR.metric <- metrics(svmR, response = "FI_q31")

mod.sum <- data.frame(rbind(
                          c("Logistic", log.metric$mcr, log.metric$accuracy, log.metric$sens, log.metric$spec, log.metric$fbeta),
                          c("LDA",  lda.metric$mcr, lda.metric$accuracy, lda.metric$sens, lda.metric$spec, lda.metric$fbeta),
                          c("LASSO", lasso.metric$mcr, lasso.metric$accuracy, lasso.metric$sens, lasso.metric$spec, lasso.metric$fbeta),
                          c("Ridge", ridge.metric$mcr, ridge.metric$accuracy, ridge.metric$sens, ridge.metric$spec, ridge.metric$fbeta),
                          # c("Bagging", bag.metric$mcr, bag.metric$accuracy, bag.metric$sens, bag.metric$spec, bag.metric$fbeta),
                          c("SVC",  svc.metric$mcr, svc.metric$accuracy, svc.metric$sens, svc.metric$spec, svc.metric$fbeta),
                          c("SVM (Radial Kernel)", svmR.metric$mcr, svmR.metric$accuracy, svmR.metric$sens, svmR.metric$spec, svmR.metric$fbeta)))

names(mod.sum) <- c("Model", "Misclassification Rate", "Accuracy", "Sensitivity", "Specificity", "fbeta")

kable(mod.sum, align = "lccccc", caption = "Table : Evaluation metrics for  food insecurity 3 as a response") %>%
  kable_paper("hover", full_width = F)%>% 
       kable_styling(font_size = 12, latex_options = c("HOLD_position"))


Var <- varImp(svmR, scale = FALSE)
plot(Var, main =
       "Variable of Importance")


```


# Discussion

Base on our table of results the best model is selected based on the performance metrics accuracy, misclassification, and fbeta. That is a model with the least misclassification, higher accuracy, and an fbeta score approaching one. Considering the results from the housing insecurity models, the SVM with the radial basis kernel has the highest f$\beta$ value and the least misclassification rate, followed by the linear discriminate analysis. Hence the support vector machine through the radial basis function is chosen as our best model. The variables are highly associated with predicting housing insecurity as displayed by the graph of the variable of important shows that total income, residence, and transportation reliability been the top variables with federal aid, gender, and weekly work hours being least of importance. However if a cutoff of say 50% is employed the best or associated variables will be the top three variables together with the head of household, living alone, and spending the night elsewhere. 

Furthermore, we observed that SVM performs better with the least misclafficattion rate, the highest accuracy, and f$\beta$ value of best performance, respectively for the food insecurity. This is associated with the response Food insecurity 1. Assuming a cut off point of 50%, we can infer from the variable importance plot that, total income,  transport reliability and household are the top variables which are associated or predicts the response variable.


Also, we can observe that, support vector machine comes with the lowest misclaffication rate, the highest accuracy, and the best f$\beta$ value closest to 1. This shows that, SVM performs better considering response variables: Food insecurity 2 and 3. Considering a cut of 50%, we can conclude from the variable importance plot that total income, transport reliability and known homelessnes student show the strongest associated with food insecurity 2 and 3. 

<!--Base on our table of results the best model is selected based on the performance metrics accuracy, misclassification, and fbeta. That is a model with the least misclassification, higher accuracy, and an fbeta score approaching one. Considering the results from the housing insecurity models, the SVM with the radial basis kernel has the highest f$\beta$ of 96% and the least misclassification rate of 0.4%, followed by the linear discriminate analysis with f$\beta$ 72% and misclassification of 3%. Hence the support vector machine through the radial basis function is chosen as our best model. The variables are highly associated with predicting housing insecurity as displayed by the graph of the variable of important shows that total income, residence, and transportation reliability been the top variables with federal aid, gender, and weekly work hours being least of importance. However if a cutoff of say 50% is employed the best or associated variables will be the top three variables together with the head of household, living alone, and spending the night elsewhere. 

Furthermore, we observed that SVM performs better with a misclafficattion rate of 0.08, accuracy of 0.92, and f$\beta$ of 0.92 respectively for the food insecurity. This is associated with the response Food insecurity 1. Using a cut of 50%, we can infer from the variable importance plot that, total income,  transport reliability and household are the top variables which is associated or predicts the response variable.
 
Also, we can observe that, support vector machine with misclaffication rate of 0.14, with accuracy of 0.86, and f$\beta$ of 0.86 shows that, SVM performs better considering response variable: Food insecurity 3. Considering a cut of 50%, we can observe infer from the variable importance plot that total income, transport reliability and known homelessnes student predict or is associated with food insecurity 3. 

Again, we can observe that, support vector machine with misclaffication rate of 0.07, with accuracy of 0.93, and f$\beta$ of 0.93 shows that, SVM performs better considering response variable: Food insecurity 2 . Considering a cut of 50%, we can observe infer from the variable importance plot that total income, transport reliability and known_homelessnes_student predict or is associated with food insecurity. We can observe that, the total income, total reliability and known_homelessness_student are associated with both food insecurity 2  and 3. -->

In a general, total income being an underlying factor for both food insecurity and housing insecurity, clearly suggests that the income on which students live for their livelihood appears not to be enough. Also, we observed issues of multiple responses with some of the variables, especially, for the gender variable. We did not find it reasonable for respondents to be given the chance to select more than one options for their gender. 

# Recommendations

From the results and discussions, we make the following recommendations:

* There should be proper advertisement of food pantries and shelters if there exist some for low income students to survive on in the absence of adequate financial aid.

* With regards to multiple responses, we are of the opinion that this should not be allowed for the gender variable in subsequent surveys.


# Future Work (Phase II)

## Rule for defining response variable for Food and Housing Insecurities ##

In the phase of the analysis, we will provide a rule for classifying a student as food insecure or housing insecure. These rules were used in selecting the appropriate response variables for both food and housing insecurities.

We hope to construct a measure of food insecurity as a dichotomous response variable that combines what we consider to be four main dimensions of food insecurity embodied in five of the survey questions, namely, a stable source of food , lack of healthy/balanced meals, inadequate size of food – eating less and going hungry . Using this definition as proposed by the USDA, we selected $Q26, Q27, Q28$ and $Q30$ respectively as our response variable for food insecurity. Therefore, an individual who participated in the survey is classified as being at risk of food insecurity if they answered yes to all of questions $Q26, Q27, Q31$, and answered yes to either $Q28$ or  $Q30$ since these two questions are similar in terms of measuring a single dimension of food insecurity. Thus, an individual who reports four conditions that indicate food insecurity are classified as “food insecure”. 


 <!-- At risk for food insecurity= $$$$ -->


# References{-}

<div id="refs"></div>

# Apendix{-}

## Visualizing Missing Data 
```{r}
# explore the patterns
gg_miss_upset(FIHI_sub3)

# explore missingness in variables with gg_miss_var
miss_var_summary(FIHI_sub3) %>% kable()

gg_miss_var(FIHI_sub3) + ylab("Number of missing values")

gg_miss_var(FIHI_sub3, show_pct = T) 
```

## Codes

Codes for the analysis is available upon request.



